{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Recovery\n",
    "\n",
    "This notebook conducts parameter recovery simulations for modified Rachlin discount function ([Vincent, & Stewart, 2019](https://doi.org/10.31234/osf.io/29sgd)).\n",
    "\n",
    "$$\n",
    "V(R, D, k) = R \\cdot \\frac{1}{1+(k \\cdot D)^s}\n",
    "$$\n",
    "\n",
    "where $R$ is a reward, delivered at a delay $D$. \n",
    "\n",
    "The parameters are:\n",
    "- $k$ is the normally interpreted as the discount rate. Although technically in this case it is the product of the discount rate and the constant term in Steven's Power Law.\n",
    "- $s$ is the exponent in Steven's Power Law.\n",
    "\n",
    "### Important note\n",
    "In order for this to be a meaningful parameter recovery excercise then the data generating model defined in `generate_responses` _must_ be exactly the same model that is used for inference in `infer_parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, bernoulli, uniform\n",
    "import pymc3 as pm\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Initialize random number generator\n",
    "np.random.seed(1234)\n",
    "\n",
    "import sys\n",
    "print(\"Python version:\\n{}\\n\".format(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters of the parameter recovery simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simulations = 200\n",
    "export = True\n",
    "\n",
    "sample_options = {'tune': 1000, 'draws': 2000,\n",
    "                  'chains': 4, 'cores': 4,\n",
    "                  'nuts_kwargs': {'target_accept': 0.95}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the core `parameter_recovery` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_recovery():\n",
    "    '''\n",
    "    Conducts parameter recovery for a single simulated experiment.\n",
    "    Return a single row of a DataFrame which contains the true and recovered\n",
    "    parameter values.\n",
    "    '''\n",
    "    params_true = generate_true_params()\n",
    "    expt_data = simulate_experiment(params_true)\n",
    "    params_inferred = infer_parameters(expt_data)\n",
    "    row_data = pd.concat([params_true, params_inferred], axis=1)\n",
    "    return row_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define these functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our set of true parameters will be generated in `generate_true_params`. These are sampled from normal distributions which equate to our prior beliefs over participant level $\\log(k)$ and $\\log(s)$ used in our parameter estimation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_true_params():\n",
    "    '''Sample ONCE from our distribution of plausible parameter values'''\n",
    "    \n",
    "    logk_min, logk_max = -5, -1.5\n",
    "    logk = uniform.rvs(loc=logk_min, scale=logk_max-logk_min) \n",
    "    \n",
    "    logs_min, logs_max = np.log(0.5), np.log(3)\n",
    "    logs = uniform.rvs(loc=logs_min, scale=logs_max-logs_min) \n",
    "    \n",
    "    return pd.DataFrame({'logk': [logk],\n",
    "                         'logs': [logs]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_experiment(params_true, 系=0.01):\n",
    "    '''Run a simulated experiment, returning simulated behavioural data'''\n",
    "    designs = generate_designs()\n",
    "    responses, _ = generate_responses(designs, params_true, 系)\n",
    "    return pd.concat([designs, responses], axis=1)\n",
    "\n",
    "\n",
    "def generate_designs():\n",
    "    '''Generate designs (RA, DA, RB, DB). This should precisely match the \n",
    "    set of questions we used in the actual experiment.'''\n",
    "    \n",
    "    n = 50\n",
    "    RA_vals = np.array([6, 12, 18, 24, 30, 36, 42, 48, 54, 60])\n",
    "    DB_vals = np.array([7, 15, 29, 56, 101])\n",
    "    \n",
    "    # define constant values\n",
    "    DA = np.zeros(n)\n",
    "    RB = np.full(n, 60)\n",
    "\n",
    "    # shuffle index for DB\n",
    "    DB_index = np.arange(len(DB_vals))\n",
    "    np.random.shuffle(DB_index)\n",
    "    \n",
    "    # fill remaining design dimensions by iterating over DB (shuffled) and RA\n",
    "    DB = []\n",
    "    RA = []\n",
    "    for db_index in DB_index:\n",
    "        for ra in RA_vals:\n",
    "            DB.append(DB_vals[db_index])\n",
    "            RA.append(ra)\n",
    "    \n",
    "    DB = np.array(DB)\n",
    "    RA = np.array(RA)\n",
    "    \n",
    "    designs = pd.DataFrame({'RA': RA, 'DA': DA, 'RB': RB, 'DB': DB})\n",
    "    return designs\n",
    "\n",
    "\n",
    "def generate_responses(designs, params_true, 系):\n",
    "    '''Generate simulated responses for the given designs and parameters'''\n",
    "    \n",
    "    # unpack designs\n",
    "    RA = designs['RA'].values\n",
    "    DA = designs['DA'].values\n",
    "    RB = designs['RB'].values\n",
    "    DB = designs['DB'].values\n",
    "    \n",
    "    # unpack parameters\n",
    "    logk = params_true['logk'].values\n",
    "    logs = params_true['logs'].values\n",
    "    \n",
    "    k = np.exp(logk)\n",
    "    s = np.exp(logs)\n",
    "    \n",
    "    VA = RA * (1 / (1 + (k*DA)**s))\n",
    "    VB = RB * (1 / (1 + (k*DB)**s))\n",
    "    decision_variable = VB-VA\n",
    "    p_choose_B = 系 + (1 - 2 * 系) * (1 / (1 + np.exp(-1.7 * decision_variable)))\n",
    "    responses = bernoulli.rvs(p_choose_B)\n",
    "    return pd.DataFrame({'R': responses}), p_choose_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_parameters(data):\n",
    "    '''Infer parameter values based on response data.\n",
    "    Return the posterior mean parameter estimates'''\n",
    "    \n",
    "    model = generate_model(data)\n",
    "    \n",
    "    # do the inference\n",
    "    with model:\n",
    "        trace = pm.sample(**sample_options)\n",
    "    \n",
    "    return extract_info_from_trace(trace)\n",
    "\n",
    "\n",
    "def generate_model(data):\n",
    "    '''Generate a PyMC3 model with the given observed data'''\n",
    "    \n",
    "    # decant data\n",
    "    R = data['R'].values\n",
    "    RA, DA = data['RA'].values, data['DA'].values\n",
    "    RB, DB = data['RB'].values, data['DB'].values\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        # define priors\n",
    "        logk = pm.Normal('logk', mu=-3.760, sd=3)\n",
    "        logs = pm.Normal('logs', mu=0, sd=1)\n",
    "        \n",
    "        VA = pm.Deterministic('VA', value_function(RA, DA, logk, logs))\n",
    "        VB = pm.Deterministic('VB', value_function(RB, DB, logk, logs))\n",
    "        P_chooseB = pm.Deterministic('P_chooseB', choice_psychometric(VB-VA))\n",
    "        \n",
    "        R = pm.Bernoulli('R', p=P_chooseB, observed=R)\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "# helper functions for the model\n",
    "\n",
    "def value_function(reward, delay, logk, logs):\n",
    "    '''Calculate the present subjective value of a given prospect'''\n",
    "    k = pm.math.exp(logk)\n",
    "    s = pm.math.exp(logs)\n",
    "    return reward / (1.0+(k*delay)**s)\n",
    "\n",
    "def choice_psychometric(x, 系=0.01):\n",
    "    # x is the decision variable\n",
    "    return 系 + (1.0-2.0*系) * (1/(1+pm.math.exp(-1.7*(x))))\n",
    "\n",
    "\n",
    "def trace_quantiles(x):\n",
    "    return pd.DataFrame(pm.quantiles(x, [2.5, 5, 50, 95, 97.5]))\n",
    "\n",
    "def extract_info_from_trace(trace):\n",
    "    '''Return a 1-row DataFrame of summary statistics (i.e. means, ranges)\n",
    "    of the parameters of interest'''\n",
    "    \n",
    "    # useful PyMC3 function to get summary statistics\n",
    "    summary = pm.summary(trace, ['logk', 'logs'])\n",
    "\n",
    "    logk = summary['mean']['logk']\n",
    "    logkL = summary['hpd_2.5']['logk']\n",
    "    logkU = summary['hpd_97.5']['logk']\n",
    "\n",
    "    logs = summary['mean']['logs']\n",
    "    logsL = summary['hpd_2.5']['logs']\n",
    "    logsU = summary['hpd_97.5']['logs']\n",
    "\n",
    "    return pd.DataFrame({'logk_est': [logk], 'logk_est_L': [logkL], 'logk_est_U': [logkU],\n",
    "                         'logs_est': [logs], 'logs_est_L': [logsL], 'logs_est_U': [logsU]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct the parameter recovery simulations\n",
    " Warning: this will take some time to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [parameter_recovery() for n in range(n_simulations)]\n",
    "\n",
    "results = pd.concat(results, ignore_index=True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export results\n",
    "results.to_csv(f'{out_dir}parameter_recovery_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of parameter recovery results\n",
    "Plot the true parameter values (x axis) against the estimated parameter values (y axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "tick_spacing = 1\n",
    "params = ['logk', 'logs']\n",
    "\n",
    "for i, parameter in enumerate(params):\n",
    "    \n",
    "    x = results[parameter]\n",
    "    y = results[f'{parameter}_est']\n",
    "    \n",
    "    yerrL = results[f'{parameter}_est'] - results[f'{parameter}_est_L']\n",
    "    yerrU = results[f'{parameter}_est_U'] - results[f'{parameter}_est']\n",
    "    \n",
    "    # line of equality\n",
    "    ax[i].plot([np.min(x), np.max(x)], [np.min(x), np.max(x)], c='k')\n",
    "    \n",
    "    # errorbar\n",
    "    ax[i].errorbar(x, y, yerr=[yerrL, yerrU], fmt='o', alpha=0.5)\n",
    "    \n",
    "    ax[i].set_xlabel(f'true {parameter}')\n",
    "    ax[i].set_ylabel(f'estimated {parameter}')\n",
    "    ax[i].grid()\n",
    "    #ax[i].set_aspect('equal', 'box')\n",
    "    \n",
    "    # set same tick spacing for both axes\n",
    "    ax[i].xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "    ax[i].yaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "if export:\n",
    "    plt.savefig(f'{out_dir}parameter_recovery.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise set of true parameters used in the parameter recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "tick_spacing = 1\n",
    "params = ['logk', 'logs']\n",
    "\n",
    "ax.scatter(x=results['logk'], y=results['logs'])\n",
    "ax.set(xlabel='true $\\log(k)$', ylabel='true $\\log(s)$')\n",
    "\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise a grid of discount functions spanning this parameter range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logk_res, logs_res = 6, 6\n",
    "\n",
    "logk_min, logk_max = -5, -1.5\n",
    "logk_vec = np.linspace(logk_min,logk_max, logk_res)\n",
    "\n",
    "logs_min, logs_max = np.log(0.2), np.log(3)\n",
    "logs_vec = np.linspace(logs_min,logs_max, logs_res)\n",
    "\n",
    "logk_matrix, logs_matrix = np.meshgrid(logk_vec, logs_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flip the row ordering so that we plot high values of s in the top rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logk_matrix = np.flipud(logk_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_matrix = np.flipud(logs_matrix)\n",
    "np.exp(logs_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_vec = np.flip(logs_vec)\n",
    "np.exp(logs_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_function(delay, k, s):\n",
    "    ''' This is the MODIFIED Rachlin discount function. This is outlined\n",
    "    in Vincent & Stewart (2018).\n",
    "    Vincent, B. T., & Stewart, N. (2018, October 16). The case of muddled \n",
    "    units in temporal discounting. https://doi.org/10.31234/osf.io/29sgd\n",
    "    '''\n",
    "    return 1 / (1.0+(k*delay)**s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig, ax = plt.subplots(logs_res, logk_res,\n",
    "                       gridspec_kw = {'wspace':0, 'hspace':0},\n",
    "                       figsize=(14, 14))\n",
    "\n",
    "max_delay = 101\n",
    "delays = np.linspace(0, max_delay, 1000)\n",
    "\n",
    "for row in range(logs_res):\n",
    "    for col in range(logk_res):\n",
    "        \n",
    "        logk = logk_matrix[row, col]\n",
    "        logs = logs_matrix[row, col]\n",
    "        \n",
    "        ax[row,col].plot(delays, \n",
    "                         discount_function(delays, np.exp(logk), np.exp(logs)),\n",
    "                         c='k')\n",
    "        ax[row,col].set(xlim=[0, 101], ylim=[0, 1])\n",
    "\n",
    "        if col > 0 or row < logs_res-1:\n",
    "            ax[row,col].set_yticklabels([])\n",
    "            ax[row,col].set_xticklabels([])\n",
    "        else:\n",
    "            ax[row,col].set_xlabel(\"delay [sec]\")\n",
    "            ax[row,col].set_ylabel(\"discount fraction\")\n",
    "            \n",
    "        ax[row,col].set_xticks(np.arange(0,101,20))\n",
    "\n",
    "# Create row titles ------------------------------------------------------------\n",
    "row_headings = [f's={np.exp(logs):.2f}' for logs in logs_vec]\n",
    "                            \n",
    "pad = 13 # in points\n",
    "for axis, row_title in zip(ax[:,0], row_headings):\n",
    "    axis.annotate(row_title, xy=(0, 0.5), xytext=(-axis.yaxis.labelpad - pad, 0),\n",
    "                  xycoords=axis.yaxis.label, textcoords='offset points',\n",
    "                  size='large', ha='center', va='center', rotation=90)\n",
    "    \n",
    "# Create col titles ------------------------------------------------------------  \n",
    "col_headings = [f'ln(k)=\\n{logk:.2f}' for logk in logk_vec]\n",
    "\n",
    "for axis, col in zip(ax[0], col_headings):\n",
    "    axis.annotate(col, xy=(0.5, 1), xytext=(0, pad),\n",
    "                  xycoords='axes fraction', textcoords='offset points',\n",
    "                  size='large', ha='center', va='baseline')\n",
    "            \n",
    "        \n",
    "plt.savefig(f'{out_dir}df_grid.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- Vincent, B. T., & Stewart, N. (2019, Jan 30th). The case of muddled units in temporal discounting. https://doi.org/10.31234/osf.io/29sgd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
