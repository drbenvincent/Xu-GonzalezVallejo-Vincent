{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Recovery\n",
    "\n",
    "This notebook conducts 2D parameter recovery simulations for modified Rachlin discount function ([Vincent, & Stewart, 2020](https://doi.org/10.1016/j.cognition.2020.104203)).\n",
    "\n",
    "$$\n",
    "V(R, D, k) = R \\cdot \\frac{1}{1+(k \\cdot D)^s}\n",
    "$$\n",
    "\n",
    "where $R$ is a reward, delivered at a delay $D$. \n",
    "\n",
    "The parameters are:\n",
    "- $k$ is the normally interpreted as the discount rate. Although technically in this case it is the product of the discount rate and the constant term in Steven's Power Law.\n",
    "- $s$ is the exponent in Steven's Power Law.\n",
    "\n",
    "**Important note:** In order for this to be a meaningful parameter recovery excercise then the data generating model defined in `generate_responses` _must_ be exactly the same model that is used for inference in `infer_parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, bernoulli, uniform\n",
    "import pymc3 as pm\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "\n",
    "# Initialize random number generator\n",
    "np.random.seed(1234)\n",
    "\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyMC3 version: {pm.__version__}\")\n",
    "\n",
    "# Install Black autoformatter with: pip install nb-black\n",
    "# %load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define options for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation options\n",
    "n_simulations = 2\n",
    "log_s_list = np.log([0.5, 1, 1.5, 2])\n",
    "log_k_list = [-5, -4, -3, -2]\n",
    "should_visualise = False\n",
    "\n",
    "# export options\n",
    "export = True\n",
    "out_dir = \"output/\"\n",
    "\n",
    "# PyMC3 inference options\n",
    "sample_options = {\n",
    "    \"tune\": 1000,\n",
    "    \"draws\": 2000,\n",
    "    \"chains\": 2,\n",
    "    \"cores\": 2,\n",
    "    \"nuts_kwargs\": {\"target_accept\": 0.95},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make 2D grid of true parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 2D grid of true parameters\n",
    "param_grid = np.zeros((len(log_s_list), len(log_k_list)), dtype=object)\n",
    "for row, logs in enumerate(log_s_list):\n",
    "    for col, logk in enumerate(log_k_list):\n",
    "        param_grid[row, col] = (logs, logk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a corresponding set of colours, one for each parameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_colours():\n",
    "    # one hue, value for each logk value (column)\n",
    "    hue_list = [19 / 360, 236 / 360, 88 / 360, 324 / 360]\n",
    "    v_list = [0.745, 0.78, 0.63, 0.72]\n",
    "\n",
    "    # one saturation for each logs value ()\n",
    "    saturation_list = np.linspace(0.2, 1.0, len(log_s_list))\n",
    "\n",
    "    cols = np.zeros((len(log_s_list), len(log_k_list)), dtype=object)\n",
    "\n",
    "    for i, saturation in enumerate(saturation_list):\n",
    "        for j, hue in enumerate(hue_list):\n",
    "            cols[i, j] = hsv_to_rgb((hue, saturation, v_list[i]))\n",
    "\n",
    "    # get list of hues (for each kappa) for the histograms\n",
    "    hue_cols = cols[-1, :]\n",
    "    return cols\n",
    "\n",
    "\n",
    "cols = make_colours()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise discount functions for these true parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_true_discount_functions(ax=None):\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    D = np.linspace(0, 100, 1000)\n",
    "\n",
    "    for row in range(len(log_s_list)):\n",
    "        for col in range(len(log_k_list)):\n",
    "            logs, logk = param_grid[row, col]\n",
    "            s, k = np.exp(logs), np.exp(logk)\n",
    "            y = 1 / (1 + (k * D) ** s)\n",
    "            ax.plot(D, y, c=cols[row, col], lw=2, label=\"true\")\n",
    "\n",
    "    ax.set(xlabel=\"delay [seconds]\", ylabel=\"$RA/RB$\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_discount_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for the inference procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: ADD ALPHA AS A FREE PARAMETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_parameters(data):\n",
    "    \"\"\"Infer parameter values based on response data.\n",
    "    Return the posterior mean parameter estimates\"\"\"\n",
    "\n",
    "    model = generate_model(data)\n",
    "\n",
    "    # do the inference\n",
    "    with model:\n",
    "        trace = pm.sample(**sample_options)\n",
    "\n",
    "    return np.array([np.mean(trace[\"logs\"]), np.mean(trace[\"logk\"])])\n",
    "\n",
    "\n",
    "def generate_model(data):\n",
    "    \"\"\"Generate a PyMC3 model with the given observed data\"\"\"\n",
    "\n",
    "    # decant data\n",
    "    R = data[\"R\"].values\n",
    "    RA, DA = data[\"RA\"].values, data[\"DA\"].values\n",
    "    RB, DB = data[\"RB\"].values, data[\"DB\"].values\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # define priors\n",
    "        logk = pm.Normal(\"logk\", mu=np.log(1 / 30), sd=3)\n",
    "        logs = pm.Normal(\"logs\", mu=0, sd=1)\n",
    "\n",
    "        VA = pm.Deterministic(\"VA\", value_function(RA, DA, logk, logs))\n",
    "        VB = pm.Deterministic(\"VB\", value_function(RB, DB, logk, logs))\n",
    "        P_chooseB = pm.Deterministic(\"P_chooseB\", choice_psychometric(VB - VA))\n",
    "\n",
    "        R = pm.Bernoulli(\"R\", p=P_chooseB, observed=R)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# helper functions for the model\n",
    "\n",
    "\n",
    "def value_function(reward, delay, logk, logs):\n",
    "    \"\"\"Calculate the present subjective value of a given prospect\"\"\"\n",
    "    k = pm.math.exp(logk)\n",
    "    s = pm.math.exp(logs)\n",
    "    return reward / (1.0 + (k * delay) ** s)\n",
    "\n",
    "\n",
    "def choice_psychometric(x, ϵ=0.01):\n",
    "    # x is the decision variable\n",
    "    return ϵ + (1.0 - 2.0 * ϵ) * (1 / (1 + pm.math.exp(-1.7 * (x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to simulate an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_experiment(params_true, ϵ=0.01):\n",
    "    \"\"\"Run a simulated experiment, returning simulated behavioural data\"\"\"\n",
    "    designs = generate_designs()\n",
    "    responses, _ = generate_responses(designs, params_true, ϵ)\n",
    "    return pd.concat([designs, responses], axis=1)\n",
    "\n",
    "\n",
    "def generate_designs():\n",
    "    \"\"\"Generate designs (RA, DA, RB, DB). This should precisely match the \n",
    "    set of questions we used in the actual experiment.\"\"\"\n",
    "\n",
    "    n = 50\n",
    "    RA_vals = np.array([6, 12, 18, 24, 30, 36, 42, 48, 54, 60])\n",
    "    DB_vals = np.array([7, 15, 29, 56, 101])\n",
    "\n",
    "    # define constant values\n",
    "    DA = np.zeros(n)\n",
    "    RB = np.full(n, 60)\n",
    "\n",
    "    # shuffle index for DB\n",
    "    DB_index = np.arange(len(DB_vals))\n",
    "    np.random.shuffle(DB_index)\n",
    "\n",
    "    # fill remaining design dimensions by iterating over DB (shuffled) and RA\n",
    "    DB = []\n",
    "    RA = []\n",
    "    for db_index in DB_index:\n",
    "        for ra in RA_vals:\n",
    "            DB.append(DB_vals[db_index])\n",
    "            RA.append(ra)\n",
    "\n",
    "    DB = np.array(DB)\n",
    "    RA = np.array(RA)\n",
    "\n",
    "    designs = pd.DataFrame({\"RA\": RA, \"DA\": DA, \"RB\": RB, \"DB\": DB})\n",
    "    return designs\n",
    "\n",
    "\n",
    "def generate_responses(designs, params_true, ϵ):\n",
    "    \"\"\"Generate simulated responses for the given designs and parameters\"\"\"\n",
    "\n",
    "    # unpack designs\n",
    "    RA = designs[\"RA\"].values\n",
    "    DA = designs[\"DA\"].values\n",
    "    RB = designs[\"RB\"].values\n",
    "    DB = designs[\"DB\"].values\n",
    "\n",
    "    # unpack parameters\n",
    "    logs, logk = params_true\n",
    "\n",
    "    k = np.exp(logk)\n",
    "    s = np.exp(logs)\n",
    "\n",
    "    VA = RA * (1 / (1 + (k * DA) ** s))\n",
    "    VB = RB * (1 / (1 + (k * DB) ** s))\n",
    "    decision_variable = VB - VA\n",
    "    p_choose_B = ϵ + (1 - 2 * ϵ) * (1 / (1 + np.exp(-1.7 * decision_variable)))\n",
    "    responses = bernoulli.rvs(p_choose_B)\n",
    "    return pd.DataFrame({\"R\": responses}), p_choose_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_experiment((np.log(1), -2.0)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(data, data_generating_params, recovered_params):\n",
    "    \"\"\"Visualise the results of a simulated experiment\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    plt.scatter(data.DB, data.RA / data.RB, c=data.R)\n",
    "\n",
    "    D = np.linspace(0, 100, 1000)\n",
    "\n",
    "    # plot recovered\n",
    "    logs, logk = recovered_params[0], recovered_params[1]\n",
    "    s, k = np.exp(logs), np.exp(logk)\n",
    "    y = 1 / (1 + (k * D) ** s)\n",
    "    plt.plot(D, y, \"r\", alpha=0.5, lw=2, label=\"recovered\")\n",
    "\n",
    "    # plot true\n",
    "    logs, logk = data_generating_params\n",
    "    s, k = np.exp(logs), np.exp(logk)\n",
    "    y = 1 / (1 + (k * D) ** s)\n",
    "    plt.plot(D, y, \"k\", lw=2, label=\"true\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def many_simulations(\n",
    "    data_generating_params, N_simulations=50, debug=False, should_visualise=True\n",
    "):\n",
    "\n",
    "    N_PARAMETERS = 2\n",
    "    recovered_params = np.empty([N_simulations, N_PARAMETERS])\n",
    "\n",
    "    for i in range(N_simulations):\n",
    "        print(f\"Simulation {i+1} of {N_simulations}\")\n",
    "        expt_data = simulate_experiment(data_generating_params)\n",
    "        recovered_params[i, :] = infer_parameters(expt_data)\n",
    "        print(recovered_params[i, :])\n",
    "        if should_visualise:\n",
    "            visualise(expt_data, data_generating_params, recovered_params[i, :])\n",
    "\n",
    "    return (recovered_params, data_generating_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((len(log_s_list), len(log_k_list)), dtype=object)\n",
    "for row, log_s in enumerate(log_s_list):\n",
    "    for col, log_k in enumerate(log_k_list):\n",
    "        params = param_grid[row,col]\n",
    "        results[row,col] = many_simulations(params, \n",
    "                                            N_simulations=n_simulations, \n",
    "                                            should_visualise=should_visualise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param_recovery(ax=None):\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    for row, _ in enumerate(log_s_list):\n",
    "        for col, _ in enumerate(log_k_list):\n",
    "\n",
    "            modified_rachlinθ, trueθ = results[row, col]\n",
    "\n",
    "            # plot inferred value\n",
    "            s = np.exp(modified_rachlinθ[:, 0])\n",
    "            logk = modified_rachlinθ[:, 1]\n",
    "            ax.scatter(x=logk, y=s, c=cols[row, col], alpha=0.4)\n",
    "\n",
    "            # plot true value\n",
    "            logs_true, logk_true = trueθ\n",
    "            s_true = np.exp(logs_true)\n",
    "            # plot true values\n",
    "            ax.scatter(x=logk_true, y=s_true, c=\"k\", label=\"true\")\n",
    "\n",
    "            ax.set_xlabel(r\"$\\log(k)$\")\n",
    "            ax.set_ylabel(r\"$s$\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the final plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(9, 14))\n",
    "\n",
    "plot_true_discount_functions(ax[0])\n",
    "plot_param_recovery(ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export:\n",
    "    fig.savefig(f'{out_dir}parameter_recovery_2D.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- Vincent, B. T., & Stewart, N. (2020). The case of muddled units in temporal discounting. _Cognition_, 198, 1-11. https://doi.org/10.1016/j.cognition.2020.104203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
